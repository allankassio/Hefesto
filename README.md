# ğŸ› ï¸ Hefesto Game Lab

Sistema automatizado para geraÃ§Ã£o de **Game Design Documents (GDDs)**, **cÃ³digo JavaScript educacional** e **artefatos desplugados**, usando **modelos de linguagem** executados localmente via Docker com [Ollama](https://ollama.com).

---

## ğŸ“ 1. Configure os modelos no `.env`

Antes de tudo, crie um arquivo `.env` na raiz do projeto e defina os modelos desejados:

```env
LLM_TEXT_MODEL=deepseek-r1
LLM_CODE_MODEL=deepseek-coder-v2
LLM_API_BASE_URL=http://llm:11434/v1
```

VocÃª pode escolher **um modelo para linguagem geral** e **um modelo para cÃ³digo**.
Lembre-se que quanto maior e mais parÃ¢metros tiver o modelo, mais recursos computacionais ele vai exigir, 
em compensaÃ§Ã£o, mais eficiente e detalhado serÃ¡ o GDD gerado.

### ğŸ”¤ Modelos de linguagem geral disponÃ­veis:

- `llama2`, `llama2:13b`, `llama2:70b`
- `llama3.1`, `llama3.2`, `llama3.3`
- `mistral`, `mistral-nemo`
- `gemma:2b`, `gemma:7b`, `gemma3:12b`, `gemma3:27b`
- `phi`, `phi4`, `phi4-mini`
- `deepseek-r1`, `deepseek-r1:671b`
- `qwen`, `qwen2`, `qwen2.5`
- `mixtral:8x7b`, `mixtral:8x22b`
- `starling-lm`, `neural-chat`, `olmo2`

### ğŸ’» Modelos para geraÃ§Ã£o de cÃ³digo:

- `codellama`
- `starcoder2`
- `qwen2.5-coder`
- `deepseek-coder-v2`

---

## ğŸ§± 2. Construa a imagem base do servidor de modelos (Ollama)

Este projeto usa um `Dockerfile.base` customizado para configurar o servidor de modelos com suporte a CUDA ou CPU.
Ã‰ importante executar esse Dockerfile antes pois ele gera uma imagem com todo recurso pesado. 
Evitando fazer download de vÃ¡rias dependÃªncias novamente.

Execute:

```bash
docker build -f Dockerfile.base -t mybase:cu12 .
```

> Isso cria a imagem `mybase:cu12`, usada pelo serviÃ§o `llm` no Docker Compose.

---

## ğŸš€ 3. Execute os serviÃ§os

VocÃª pode rodar **os serviÃ§os separadamente** ou **todos juntos** com o Docker Compose:

### âœ… Rodar tudo junto (recomendado):

```bash
docker compose up --build
```

### ğŸ”§ Ou rodar separadamente:

- Primeiro, iniciar o servidor de modelos:

  ```bash
  docker compose up --build llm
  ```

- Em outro terminal, iniciar o app Streamlit:

  ```bash
  docker compose up --build app
  ```

---

## ğŸŒ Acessar a aplicaÃ§Ã£o

Depois de iniciar os serviÃ§os, abra o navegador em:

```
http://localhost:8501
```

---

## ğŸ§© Funcionalidades

- GeraÃ§Ã£o de GDDs com base em pilares do pensamento computacional
- GeraÃ§Ã£o de cÃ³digo JavaScript educacional com modelos especializados
- GeraÃ§Ã£o de artefatos desplugados para atividades offline
- Download dos arquivos gerados (`.md`, `.js`, `.txt`)
- BotÃµes para gerar novamente, voltar e reiniciar
- SeleÃ§Ã£o de modelos por `.env` sem editar cÃ³digo

---

Claro! Aqui estÃ¡ um trecho institucional que vocÃª pode incluir no final do `README.md`:

---

## ğŸ“ Sobre a Pesquisa

Este sistema faz parte da pesquisa de doutorado de **Allan Kassio Beckman Soares da Cruz**, 
desenvolvida no **Programa de PÃ³s-GraduaÃ§Ã£o em CiÃªncia da ComputaÃ§Ã£o da Universidade Federal do MaranhÃ£o (DCCMAPI/UFMA)**, 
sob orientaÃ§Ã£o do **Professor Dr. Carlos de Salles Soares Neto**.

O objetivo da pesquisa Ã© investigar o uso de **modelos de linguagem (LLMs)** e **experiÃªncias gamificadas** como apoio 
ao ensino de **pensamento computacional**, com foco em **prototipaÃ§Ã£o automÃ¡tica de jogos educacionais** e **artefatos instrucionais**.